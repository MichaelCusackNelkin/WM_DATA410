<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Project_5.md</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h2 id="project-5-regularization-and-variable-selection-with-ridge-lasso-elastic-net-square-root-lasso-and-scad.">Regularization and Variable Selection with Ridge, Lasso, Elastic Net, Square Root Lasso, and SCAD.</h2>
<h3 id="part-one-sklearn-compliant-scad-and-sqrtlasso">Part One: Sklearn Compliant SCAD and SQRTLasso</h3>
<p>During this project I wrote SKLearn-compliant SCAD and SQRTLasso implementations from scratch. 
	These were well functioning classes and work well with SKLearn, but the SQRT Lasso code is particularly slow. 
	SQRTLasso code below:</p>
<pre><code>class SQRTLasso:
	def __init__(self, alpha=0.01):
		self.alpha = alpha
	
	def fit(self, x, y):
		alpha=self.alpha
		def f_obj(x,y,beta,alpha):
		n =len(x)
		beta = beta.flatten()
		beta = beta.reshape(-1,1)
		output = np.sqrt(1/n*np.sum((y-x.dot(beta))**2)) + alpha*np.sum(np.abs(beta))
		return output
		
		def f_grad(x,y,beta,alpha):
		n=x.shape[0]
		p=x.shape[1]
		beta = beta.flatten()
		beta = beta.reshape(-1,1)
		output = np.array((-1/np.sqrt(n))*np.transpose(x).dot(y-x.dot(beta))/np.sqrt(np.sum((y-x.dot(beta))**2))+alpha*np.sign(beta)).flatten()
		return output
		
		def objective(beta):
		return(f_obj(x,y,beta,alpha))

		def gradient(beta):
		return(f_grad(x,y,beta,alpha))
		
		beta0 = np.ones((x.shape[1],1))
		output = minimize(objective, beta0, method='L-BFGS-B', jac=gradient,options={'gtol': 1e-8, 'maxiter': 50000,'maxls': 25,'disp': True})
		beta = output.x
		self.coef_ = beta
		
	def predict(self, x):
		return x.dot(self.coef_)
</code></pre>
<h3 id="part-two-generating-random-data">Part Two: Generating Random Data</h3>
<p>I made the data using the below formula and order of ground truth (beta_star). I generated 100 datasets, each one normally pulling from a 200 x 1200 Toeplitz matrix where the correlations between features i and j were 0.8^{|i - j|. There were only 27 non-zero weights in the ground truth beta_star. The code I used to generate the data is shown below.</p>
<pre><code># The desired mean values of the sample.
n = 200
p= 1200

Xdata = []
ydata = []

beta_star = np.concatenate(([1]*7, [0]*25, [0.25]*5, [0]*50, [0.7]*15, [0]*1098))

mu = [0]*p
sigma = 3.5
np.random.seed(123)

v = []
for i in range(p):
    v.append(0.8**i) #1D array (vector) of 0.8**i
r = toeplitz(v)
for i in range(0,100):
	X = np.random.multivariate_normal(mu, r, size=n)
	y = X.dot(beta_star) + sigma*np.random.normal(loc=0,scale=1,size=n)
	
Xdata.append(X)
ydata.append(y)
</code></pre>
<h3 id="part-three-evaluation">Part Three: Evaluation</h3>
<p>To evaluate these  models, I graphed the L2 distance and MSE, and correctly identified non-zero weights for each model over all 100 datasets, as well as their averages.</p>
<p>Before doing this however, I used GridSearchCV to tune the major hyperparameters for each type of model. I did this on only one dataset because it took too long to run for all 100 and for each of the models. However, this did provide some major improvements.</p>
<p>I also used a validation funciton to do KFold validation for each model on all 100 datasets and find the mean squared error on each. Again because of hardware constraints, I had to set <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">k=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal" style="margin-right: 0.03148em;">k</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span></span></span></span></span> for the kfold validation. Normally I would set it higher, but this took much too long to run.</p>
<p>The code I used to for grid search, graphing, and validation for Elastic Net is below:</p>
<pre><code>grid = GridSearchCV(estimator=ElasticNet(fit_intercept=False, max_iter=10000), cv=10, scoring='neg_mean_squared_error',
				    param_grid={'alpha':np.linspace(0,1,50),'l1_ratio':np.linspace(0,1,num=50)})
gs_results = grid.fit(Xdata[0],ydata[0])
print(gs_results.best_params_)
print('The mean square error is for Elastic Net: ', np.abs(gs_results.best_score_))

# Elastic Net
non_zeros = []
L2s = []
MSEs = []
for i in range(0,100):
    model = ElasticNet(alpha=0.1, fit_intercept=False,
					    max_iter=10000)
	model.fit(Xdata[i],ydata[i].reshape(-1,1))
	beta_hat = model.coef_

	non_zeros.append(len(np.intersect1d(np.where(beta_star &gt; 0.1), np.where(beta_hat != 0))))
	L2s.append(np.linalg.norm(beta_hat - beta_star, ord=2))
	MSEs.append(validate(model, Xdata[i], ydata[i].reshape(-1,1), 2, 123))
	
	fig, (non_zero, L2, MSE) = plt.subplots(1,3, figsize=(18,6))
	
	non_zero.plot(range(0,100), non_zeros, color='blue')
	non_zero.set(xlabel='Index of Dataset',ylabel = 'Number of Correct Non-Zero Features')
	
	L2.plot(range(0,100), L2s, color = 'red')
	L2.set(xlabel='Index of Dataset',ylabel = 'L2 Distance Between Model Coefs and Ground Truth')
	
	MSE.plot(range(0,100), MSEs, color='green')
	MSE.set(xlabel='Index of Dataset',ylabel = 'Mean Squared Error of Predicted Data')
	
	plt.show()
	print('Mean correct non-zeros ' + str(np.mean(non_zeros)))
	print('Mean L2 ' + str(np.mean(L2s)))
	print('Mean MSE ' + str(np.mean(MSEs)))
</code></pre>
<p>In the end, I wasn't able to get meaningful results for all 100 datasets for SQRTLasso. Unfortunately, this homemade function would not run in time for my hardware to complete.</p>
<p>For the other models however, I got promising results.</p>
<ul>
<li>Elastic Net:
<ul>
<li>Mean correct non-zeros 24.79</li>
<li>Mean L2 2.751559123799759</li>
<li>Mean MSE 23.742791628966092</li>
<p><img src="https://michaelcusacknelkin.github.io/WM_DATA410/Projects/Data/ElasticNet.png" alt="Graphs for ElasticNet">
</p>
</ul>
</li>
<li>Lasso
<ul>
<li>Mean correct non-zeros 20.39</li>
<li>Mean L2 3.5852023017970707</li>
<li>Mean MSE 23.698891039902524</li>
<p><img src="https://michaelcusacknelkin.github.io/WM_DATA410/Projects/Data/Lasso.png" alt="Graphs for Lasso"></p>
</ul>
</li>
<li>Ridge
<ul>
<li>Mean correct non-zeros 27.0</li>
<li>Mean L2 3.167209794939203</li>
<li>Mean MSE 52.92163236150477</li>
<p><img src="https://michaelcusacknelkin.github.io/WM_DATA410/Projects/Data/Ridge.png" alt="Graphs for Ridge"></p>
</ul>
</li>
<li>SCAD
<ul>
<li>Mean correct non-zeros 27.0</li>
<li>Mean L2 3.4014219138641537</li>
<li>Mean MSE 75.63356371150178</li>
<p><img src="https://michaelcusacknelkin.github.io/WM_DATA410/Projects/Data/SCAD.png" alt="Graphs for SCAD"></p>
</ul>
</li>
</ul>
<p>As we can see, Ridge and SCAD were able to identify all 27 non-zero weights in all datasets, but had higher MSE than the other models. The lowest L2 was ElasticNet, at 2.75, while the others were each around 3.5. This seems to indicate that different model varieties perform differently across validation techniques.</p>
</div>
</body>

</html>
