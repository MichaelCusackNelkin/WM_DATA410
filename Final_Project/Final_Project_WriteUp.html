<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Actual_final.md</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="title">Traffic Intensity Video Classification</h1>
<h2 id="introduction">Introduction</h2>
<p>In an effort to reduce atmospheric pollution in, New York City has begun penalizing commercial vehicles idling for a continuous three minutes or longer. However, it would take an excessize amount of resources to catch all trucks in the city of nine million. So, NYC allows residents to report a three minute video of a commercial vehicle idling in the street and receive a small portion of the fines. A valid video must contain The trucking company's logo "noticeable" evidence that the vehicle is idling. A given redident could expect around $80 per video, and NYC has thousands of idling trucks all over the city at any given time, yet very few trucks are penalized this way because of the huge amount time required by government employees watching and verifying each video. Below I investigate the viability of an application that identifies a commercial vehicle as idleing, and records the trucking company's name using computer vision. I begin development of a video classifcation algorithm using a <a href="https://www.kaggle.com/datasets/aryashah2k/highway-traffic-videos-dataset">Highway Traffic Videos Dataset</a> from Kaggle. This will begin the first phase of my undergraduate project, learning how to classify videos.</p>
<h2 id="description-of-data">Description of Data</h2>
<p>This dataset is a video classification problem where the target is the intensity of traffic on the I-5 highway near Seattle. Each video was taken by cameras viewing the highway from above, from 08/05/2004 to 08/06/2004, and they are all approximately 52 frames long. I chose this data because:</p>
<ol>
<li>It is a video dataset, which I can use to practice building a video classification pipeline.</li>
<li>It has a high usability score on Kaggle. This means I shouldn't have to mess with the data too much, and can focus my time on learning how to accurately classify videos.</li>
<li>It is a 3-class classifcation problem (light, medium, or heavy traffic) involving cars/automobiles. This is similar to the data I would eventually use for the idling truck-catcher, which would be a 2-class problem with commerical vehicles.</li>
</ol>
<p>As this dataset is 63MB of video data (254 videos), I used Google Colab Pro GPUs to accelerate training and hyperparameter tuning. Without this, none of the below methods would've run in a amount of time.</p>
<h2 id="description-of-all-the-methods-applied">Description of All the Methods Applied</h2>
<h3 id="preprocessing-methods">Preprocessing Methods</h3>
<p>This data came relatively clean. There are the 254 videos in <em>.AVI</em> format, and there is tab-separated <em>info.txt</em> file containing video names, class labels, and metadata such as the date and weather conditions. However, there is a serious class imbalance in this dataset. Of the 254 videos 165, or 65%, are light traffic. The other 35% is split evenly between medium and heavy traffic. This heavy weighting towards light traffic makes it very important to use a stratified kfold validation later on so that train/test sets are evenly weighted between each class during model validation.</p>
<p>First I used a stratified train-test-split method to split the data into 80% train, and 20% test. Making sure to stratify the data was important because it ensured that the train and test sets contained the same proportions of classes.</p>
<p>The only preprocessing that I did was use a feature extractor to help the models train more effectively. In this case, I used the open-source InceptionV3 feature extractor, which is a convolutional neural network takes each frame of each video and “masks” it with what it believes to be important features. The model outputs the masks for each frame, each mask containing 2048 features for the frame.</p>
<h3 id="machine-learning-methods">Machine Learning Methods</h3>
<p>Here I made use of a recurrent nerual network (RNN) architecture. This decision was made becasue RNNs demonstrate the ability to "remember" data, or recognize sequences of data as related. This is especially useful here since videos are just one representation of a sequence of images related throuhg time.</p>
<p>Recurrent neural networks are a cutting edge variety of neural network that, for lack of better words, is able to retain a “memory” of the data it recently processed. In this case I used GRU layers, which take into account their most recent few outputs in determining what their current uotput will be. This is iportant for my data because within one video, the model should not be flipping between light, medium, and heavy traffic. Ideally, the model will quickly realize that the video is a specific class, and hold that prediction for all the other frames.</p>
<p>I also spent quite a lot of time on hyperparameter tuning my models. I used the keras Hyperband tuner. Below is my model architecture including hypertuning.</p>
<pre><code>class_vocab = label_processor.get_vocabulary()
frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")
x = keras.layers.GRU(units=hp.Int(name='GRU_1', min_value=12, max_value=24,
					 step=2),return_sequences=True)(frame_features_input,mask=mask_input)
x = keras.layers.GRU(units=hp.Int(name='GRU_2', min_value=2,
				     max_value=12,step=2))(x)
#Now dense/dropouts
x = keras.layers.Dropout(rate=hp.Float(name='dropout_rate',
						 min_value=0.2,max_value=0.4, step=0.1))(x)
x = keras.layers.Dense(units=hp.Int(name='dense_1', 
					  min_value=4, max_value=12, step=2),activation="relu")(x)
#Outputs and compiling the model 
output = keras.layers.Dense(len(class_vocab), activation="softmax")(x)
rnn_model = keras.Model([frame_features_input, mask_input], output)
rnn_model.compile(loss="sparse_categorical_crossentropy",
optimizer=keras.optimizers.Adam(), metrics=["accuracy"])
</code></pre>
<p>This model is comprised of an input layer, two GRU layers, a dropout layer, dense layer, and output dense layer. Everywhere there is an <code>hp</code> means I am calling the Hyperband tuner to tune that parameter in the min and max range given. For example, <code>Dropout(rate=hp.Float(name='dropout_rate', 							 min_value=0.2,max_value=0.4, step=0.1))</code> will tune the rate parameter in the dropout layer with floats between 0.2 and 0.4, in steps of 0.1 .</p>
<p>I ended up making six hypertuned models, and found them to be somewhat inconsistent. In the end, the best hypertuned model had the below configuration:</p>

<table>
<thead>
<tr>
<th>Layer</th>
<th>Tuned Parameter Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>GRU1</td>
<td>14 neurons</td>
</tr>
<tr>
<td>GRU2</td>
<td>12 neurons</td>
</tr>
<tr>
<td>Dropout</td>
<td>rate = 0.4</td>
</tr>
<tr>
<td>Dense</td>
<td>8  neurons</td>
</tr>
</tbody>
</table><h3 id="training-evaluation-and-comparison">Training, Evaluation, and Comparison</h3>
<p>In order to train, test, and validate my models, I saved them and wrote a few helper functions. Here is an example of loading my best models and testing them.</p>
<pre><code>	#Rebuild tuned_model_4, its weights and the optimizer
	model_4=tf.keras.models.load_model("/content/drive/MyDrive/DATA410_Final_Project/Tuned_Models/Tuned_model_4.h5")
	model_0=tf.keras.models.load_model("/content/drive/MyDrive/DATA410_Final_Project/Tuned_Models/Tuned_model_0.h5")
	
	#Evaluate both models
	_, acc_0 = model_0.evaluate(test_data, test_labels)
	print(f"Test accuracy: {round(accuracy * 100,  2)}%")
	
	_, acc_4 = model_4.evaluate(test_data, test_labels)
	print(f"Test accuracy: {round(accuracy * 100,  2)}%")
	
	#Collect prediction probabilities and make heatmaps of confusion matrices
	probs_0 = model_0.predict(test_data, batch_size=BATCH_SIZE)
	probs_4= model_4.predict(test_data, batch_size=BATCH_SIZE)
	Plot_Heatmaps(probs_0,probs_4)
</code></pre>
<figure>
<center>
<img src="./Data/tuned_vs_untuned.png" width="1800px">
<figcaption>Confusion Matrix of a tuned model vs an untuned model<figcaption></figcaption></figcaption></center>
</figure>
<p>In this case, model_4 performed <em>very</em> well. However, this level of performance could indicate over-fitting, especially with such an imbalance toward the "light" class in the training data. Below I use kfold validation to mitigate this risk.</p>

<h3 id="stratified-kfold-validation">Stratified KFold Validation</h3>
<p>In order to validate my models across all the data I ran a stratified kfold validation. I designed the validation loop myself, using the full dataset, but making sure to isolate the test set.  Here is the loop I ran on my two best models.</p>
<pre><code>	accs_0 = []
	accs_4 = []
	path_0 = "/content/drive/MyDrive/DATA410_Final_Project/Tuned_Models/Tuned_model_0.h5"
	path_4 = "/content/drive/MyDrive/DATA410_Final_Project/Tuned_Models/Tuned_model_4.h5"
	
	#this is the random state cross-validation loop to make sure our results are real
	for i in  range(12345,12356):
		print('Random State: ' + str(i))
		kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=i)
		for idxtrain, idxtest in kf.split(train_data[0],train_labels.flatten()):
			#Split the train and test data
			xtrain = (kfold_data[0][idxtrain],kfold_data[1][idxtrain])
			ytrain = kfold_labels[idxtrain]
			ytest = kfold_labels[idxtest]
			xtest = (kfold_data[0][idxtest],kfold_data[1][idxtest])
			
			#Model 0
			model_0=tf.keras.models.load_model(path_0)
			model_0.fit(xtrain, ytrain,
				    validation_split=0.2, epochs=25, verbose=0)
			#test the model and get the test accuracy
			_, acc_0 = model_0.evaluate(xtest, ytest, verbose=0)
			
			#Model 4
			model_4=tf.keras.models.load_model(path_4)
			model_4.fit(xtrain, ytrain,
				    validation_split=0.2, epochs=25, verbose=0)
			#test the model and get the test accuracy
			_, acc_4 = model_4.evaluate(xtest, ytest, verbose=0)
			
			#Append each accuracy
			accs_0.append(acc_0)
			accs_4.append(acc_4)
</code></pre>
<p>The above loop stratifies the full dataset into five train and test folds, making sure each fold as the same proportion of each class. It then trains each model on the train fold, tests it on the test fold, and appends the test accuracies to pre-initialized lists. Then we repeat this process 10 times for each of 10 different random states. Ideally, this validation would happen with much more data, and would be run for as many random states as possible. Below I show a histogram of the accuracies from these validation loops.</p>
<figure>
<center>
<img src="Data/0_vs_4.png" width="1800px">
<figcaption>55 KFold Accuracies for model_0 and model_4<figcaption></figcaption></figcaption></center>
</figure>
After seeing the histogram of accuracies I am more confident that model_4 really does have a good grasp of the data! It reliably reaches relatively high accuracies, given the limited nature of the data.
<h2 id="discussion-and-inferences">Discussion and Inferences</h2>
<p>As the goal of this paper is to be a test case for a future video classifier, I am very happy with these results. I never thought that I would be able to converge on such great video-classification algorithm so quickly. Next steps for this project are to start introducing audio classification in parallel, so that a future version would be able to identify a truck that is actively idling, not just whether a truck is in the frame. Of course, this model could go much further with more data, and so another important step is to collect real-world audio-video data of idling trucks.</p>
<h2 id="references">References</h2>
<ol>
<li>Video Classification with a CNN-RNN Architecture by <a href="https://twitter.com/RisingSayak">Sayak Paul</a> <a href="https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/video_classification.ipynb#scrollTo=7-4EPFk5-n_J">https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/video_classification.ipynb#scrollTo=7-4EPFk5-n_J</a></li>
<li>highway_view_some_frames by LESHABIRUKOV, <a href="https://www.kaggle.com/code/leshabirukov/highway-view-some-frames">https://www.kaggle.com/code/leshabirukov/highway-view-some-frames</a></li>
</ol>
</div>
</body>

</html>
