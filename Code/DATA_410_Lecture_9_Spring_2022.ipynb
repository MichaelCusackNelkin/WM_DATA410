{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SOqMlRibziB"
      },
      "source": [
        "# DATA 410 Lecture 9 - Spring 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDb8bGUrbV3Y"
      },
      "source": [
        "<font face=\"Chalkboard\" color=\"darkgreen\" size=10> Multivariate Models for Regression</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkd4wL1JCMKj"
      },
      "source": [
        "## Multivariate Models\n",
        "\n",
        "In general we want\n",
        "\n",
        "$$ \\mathbb{E}(y):=F(x_1,x_2,x_3,...x_p)$$\n",
        "\n",
        "where $F$ represents the model (regressor) we consider.\n",
        "\n",
        "### Variable Selection\n",
        "\n",
        "- We want to select only the features that are really important for our model.\n",
        "\n",
        "- If the functional input-output model is $Y = F(X_1,X_2,X_3,X_4,X_5...X_p)$ then we imagine that it is very possible that only a subset of the variables $X_1,X_2,X_3,X_4,X_5...X_p$ are important and we need to disconsider (eliminate from the model) those that are not relevant.\n",
        "\n",
        "- Programming and algorithms are based on equations, functions and statement evaluations.\n",
        "\n",
        "- To represent variable selection in a functional way, we can think of multiplying each variable from the model by a binary weight, a weight of $0$ means the feature is not important and a weight of $1$ means that it is important:\n",
        "\n",
        "$$\n",
        "Y = F(w_1\\cdot X_1,w_2\\cdot X_2,w_3\\cdot X_3,w_4\\cdot X_4,w_5\\cdot X_5...w_p\\cdot X_p)\n",
        "$$\n",
        "\n",
        "where the weights $w_i$ are either $0$ or $1.$\n",
        "\n",
        "The vector of binary weights $w=(w_1,w_2,w_3,...w_p)$ gives us what we call the ***sparsity pattern*** for the variable selection.\n",
        "\n",
        "### Critical Aspects\n",
        "\n",
        "1. What is the simplest choice for the function $F$?\n",
        "2. How do we perform variable selection?\n",
        "3. How do we accomodate nonlinear relationships?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCd-dl1RP66h"
      },
      "source": [
        "## Variable Selection\n",
        "\n",
        "In the case of multiple linear regression we have that\n",
        "\n",
        "$$F(X_1,X_2,...X_p)=\\beta_1 X_1+\\beta_2 X_2 + ...\\beta_p X_p$$\n",
        "\n",
        "and the sparsity pattern means that a subset of the $\\beta_1, \\beta_2, ...\\beta_p$ are equal to $0.$ \n",
        "\n",
        "So we assume \n",
        "\n",
        "$$ Y \\approx X\\cdot \\beta +\\sigma\\epsilon $$\n",
        "\n",
        "and we want the coefficients $\\beta.$ \n",
        "\n",
        "The \"classical\" way of solving is:\n",
        "\n",
        "$$X^{t}\\cdot Y \\approx X^{t}X\\cdot \\beta + \\sigma X^{t}\\epsilon$$ so we get $$ \\mathbb{E}(\\beta) = (X^{t}X)^{-1} X^{t}\\cdot \\mathbb{E}(Y)$$\n",
        "\n",
        "where $\\mathbb{E}(Y)$ denotes the expected value of $Y.$\n",
        "\n",
        "The questions that we explore are:\n",
        "\n",
        " - Why and how we know that we need variable selection.\n",
        " \n",
        " - How we measure the effects of variable selection on the model.\n",
        " \n",
        " - How to determine if the method of selecting a sparsity pattern is working in the context of our data. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's assume that we have some data with three features, such as\n",
        "\n",
        "  Housing Area  |     Value   |    Property Tax  | Sales Price |\n",
        " -------------  |    ------   |   -------------  | ----------- |\n",
        "          1800  |       234   |             9.8  |       267.5\n",
        "          1980  |       244   |            10.5  |       278.2\n",
        "          2120  |       252   |            16.2  |       284.5\n",
        "          2500  |       280   |            18.4  |       310.4\n",
        "\n",
        "\n",
        "If this is the only data we have for these features do we need to do any variable selection for predicting the sales price with a linear model?\n",
        "\n",
        "\n",
        "\n",
        "If we try to fit the *Ordinary Least Squares* model (OLS) to determine the best fit what would we do?\n",
        "\n",
        "```r\n",
        "X <- matrix(c(1800,1980,2120, 2500, 234,244,252,280,9.8,10.5,16.2,18.4),4,3)\n",
        "Y <- c(267.5,278.2,284.5,310.4)\n",
        "model <- lm(Y~X)\n",
        "model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJmTp3nUym6A"
      },
      "source": [
        "## Gradient Boosting\n",
        "\n",
        "Assume you have an regressor $F$ and, for the observation $x_i$ we make the prediction $F(x_i)$. To improve the predictions, we can regard $F$ as a 'weak learner' and therefore train a decision tree (we can call it $h$) where the new output is $y_i-F(x_i)$. Thus, there are increased chances that the new regressor\n",
        "\n",
        "$$\\large F + h$$ \n",
        "\n",
        "is better than the old one, $F.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1NhfDMt1bVyx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.linalg import lstsq\n",
        "from scipy.sparse.linalg import lsmr\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d, griddata, LinearNDInterpolator, NearestNDInterpolator\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold, train_test_split as tts\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# imports for creating a Neural Network\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.optimizers import Adam # they recently updated Tensorflow\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tricubic Kernel\n",
        "def Tricubic(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,70/81*(1-d**3)**3)\n",
        "\n",
        "# Quartic Kernel\n",
        "def Quartic(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,15/16*(1-d**2)**2)\n",
        "\n",
        "# Epanechnikov Kernel\n",
        "def Epanechnikov(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,3/4*(1-d**2))\n",
        "\n",
        "#Defining the kernel local regression model\n",
        "\n",
        "def lw_reg(X, y, xnew, kern, tau, intercept):\n",
        "    # tau is called bandwidth K((x-x[i])/(2*tau))\n",
        "    n = len(X) # the number of observations\n",
        "    yest = np.zeros(n)\n",
        "\n",
        "    if len(y.shape)==1: # here we make column vectors\n",
        "      y = y.reshape(-1,1)\n",
        "\n",
        "    if len(X.shape)==1:\n",
        "      X = X.reshape(-1,1)\n",
        "    \n",
        "    if intercept:\n",
        "      X1 = np.column_stack([np.ones((len(X),1)),X])\n",
        "    else:\n",
        "      X1 = X\n",
        "\n",
        "    w = np.array([kern((X - X[i])/(2*tau)) for i in range(n)]) # here we compute n vectors of weights\n",
        "\n",
        "    #Looping through all X-points\n",
        "    for i in range(n):          \n",
        "        W = np.diag(w[:,i])\n",
        "        b = np.transpose(X1).dot(W).dot(y)\n",
        "        A = np.transpose(X1).dot(W).dot(X1)\n",
        "        #A = A + 0.001*np.eye(X1.shape[1]) # if we want L2 regularization\n",
        "        #theta = linalg.solve(A, b) # A*theta = b\n",
        "        beta, res, rnk, s = lstsq(A, b)\n",
        "        yest[i] = np.dot(X1[i],beta)\n",
        "    if X.shape[1]==1:\n",
        "      f = interp1d(X.flatten(),yest,fill_value='extrapolate')\n",
        "    else:\n",
        "      f = LinearNDInterpolator(X, yest)\n",
        "    output = f(xnew) # the output may have NaN's where the data points from xnew are outside the convex hull of X\n",
        "    if sum(np.isnan(output))>0:\n",
        "      g = NearestNDInterpolator(X,y.ravel()) \n",
        "      # output[np.isnan(output)] = g(X[np.isnan(output)])\n",
        "      output[np.isnan(output)] = g(xnew[np.isnan(output)])\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Gradient Boosted Version of LWR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def boosted_lwr(X,y,xtest,kern,tau,intercept):\n",
        "    #we need decision trees. For training the boosted lwr we use X and y (X is the full data, used as an xtrain matrix)\n",
        "    #then once we train, we apply the boosting algorithm to new data\n",
        "    Fx = lw_reg(X,y,X,Tricubic,tau,intercept) #output of lwr, used for training the decision trees\n",
        "    #Now train the tree on yi - Fx (difference between real and regressor predictions)\n",
        "    new_y = y - Fx\n",
        "    tree_model = DecisionTreeRegressor(max_depth=3, random_state=123) #other hyperparameters currently at default\n",
        "    tree_model.fit(X,new_y)\n",
        "    output = lw_reg(X,y,xtest,kern,tau,intercept) + tree_model.predict(xtest)\n",
        "    return output\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Cross-validated MSE for Boosted multi dimensional Loess was: 20.669118972100534\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"Data/cars.csv\")\n",
        "X = data[[\"CYL\",\"ENG\",\"WGT\"]].values\n",
        "y = data['MPG'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#KFold between to see how effective each is\n",
        "mse_lw = []\n",
        "\n",
        "model_nn = Sequential()\n",
        "model_nn.add(Dense(64, activation=\"relu\", input_dim=3))\n",
        "model_nn.add(Dense(32, activation=\"relu\"))\n",
        "model_nn.add(Dense(8, activation=\"relu\"))\n",
        "model_nn.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "model_nn.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-3, decay=1e-3 / 200))\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1000)\n",
        "\n",
        "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
        "\n",
        "for idx_train, idx_test in kf.split(X):\n",
        "    xtrain = scaler.fit_transform(X[idx_train])\n",
        "    xtest = scaler.fit_transform(X[idx_test])\n",
        "\n",
        "    ytrain = y[idx_train]\n",
        "    ytest = y[idx_test]\n",
        "\n",
        "    yhat_lw = boosted_lwr(xtrain,ytrain,xtest,Tricubic,1,intercept=True)\n",
        "    \n",
        "    mse_lw.append(mse(ytest, yhat_lw))\n",
        "\n",
        "    \n",
        "print(\"The Cross-validated MSE for Boosted multi dimensional Loess was: \" + str(np.mean(mse_lw)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8d2n9QVbu3r"
      },
      "outputs": [],
      "source": [
        "X = np.transpose(np.array([[1800,1980,2120, 2500], [234,244,252,280],[9.8,10.5,16.2,18.4]]))\n",
        "Y = np.array([267.5,278.2,284.5,310.4]).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N8wSPcTb85-"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data=X,columns=['Housing Area','Value','Property Tax'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMDXCcyldoHg",
        "outputId": "b16a8c62-fca6-42d1-efc5-de5d7fcb16bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.022219667101180705"
            ]
          },
          "execution_count": 51,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.linalg.det(np.matmul(np.transpose(X),X)) # we yay !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZNgKayafAPH",
        "outputId": "c801c917-b0d6-4ea7-9f89-b07b91d7b1b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm  = LinearRegression()\n",
        "lm.fit(X,Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvqM_GTofKfY",
        "outputId": "501c87af-ce3d-4b98-8433-e0de3bf3497e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.03193903,  0.52413576, -0.41483344]])"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVJaNooSfKjb",
        "outputId": "bf2bab44-8d17-4eda-95d6-de4ee8655965"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([91.42734129])"
            ]
          },
          "execution_count": 36,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Very important: the predictions should be made only in the range of the data\n",
        "lm.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYc0HtDYfKlV",
        "outputId": "8f4739c9-eca2-4bf6-c537-a11de0e307fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[276.11998743]])"
            ]
          },
          "execution_count": 38,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm.predict([[2000,240,12]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfHZ1OrBfKrM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMQEucAmGDwc"
      },
      "source": [
        "\n",
        "## Discussion about intercept (with Python code)\n",
        "\n",
        "A simpler example (below) in Python demonstrates the intercept based on the \"mtcars\" data set where we can use the line of best fit (also known as *Ordinary Least Squares regression*) in order to predict the mileage (mpg) by using the weight feature (wt). The weights of cars are measured in tons.\n",
        "\n",
        "```python\n",
        "# Libraries of functions need to be imported\n",
        "# Then we can import the data that was saved in the same folder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "\n",
        "cars = pd.read_csv(\"mtcars.csv\")\n",
        "\n",
        "cars.head(5)\n",
        "\n",
        "x = cars[\"wt\"]\n",
        "y = cars[\"mpg\"]\n",
        "lm = linear_model.LinearRegression()\n",
        "model = lm.fit(x,y)\n",
        "\n",
        "# the following are the slope and the y-intercept \n",
        "slope = lm.coef_\n",
        "y_intercept = lm.intercept_\n",
        "\n",
        "x_range = np.arange(7)\n",
        "\n",
        "yhat  = lm.predict(x_range.reshape(-1,1))\n",
        "yhatd = lm.predict(x)\n",
        "dy = y.values - yhatd\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x_range.reshape(-1,1), yhat, '-',color='red')\n",
        "ax.scatter(cars[\"wt\"].values,cars[\"mpg\"].values,s=30,facecolors='none',edgecolors='blue')\n",
        "ax.vlines(x.values,y.values,y.values+dy,lw=0.5)\n",
        "ax.set_xlim(0, 6)\n",
        "ax.set_ylim(0, 50)\n",
        "ax.set_xlabel('Weight (1unit = 1000lbs)',fontsize=14)\n",
        "ax.set_ylabel('Miles Per Gallon',fontsize=14)\n",
        "ax.grid(b=True,which='major', color ='grey', linestyle='-', alpha=0.8)\n",
        "ax.grid(b=True,which='minor', color ='grey', linestyle='--', alpha=0.2)\n",
        "ax.minorticks_on()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "OLS works by minimizing the sum of the squared residuals in order to determine the slope and the intercept:\n",
        "\n",
        "$$\n",
        "\\text{minimize} \\sum_{i=1}^{n}\\left(y_i -n - m\\cdot x_i\\right)^2\n",
        "$$\n",
        "\n",
        "Evidently, this is the case of a univariate input (only one feature as input) and a univariate output and the predicted values are $\\hat{y}_i=m\\cdot x_i+n.$\n",
        "\n",
        "\n",
        "How many *weights* (also known as coefficients) did we estimate in total? The answer is 4 (3 features plus an intercept).\n",
        "\n",
        "- We care of having an intercept because the data was not standardized, such as the mean of each feature column is not $0.$\n",
        "\n",
        "So why exactly we know that *OLS* was supposed to work?\n",
        "\n",
        "- We compute the determinant of $X^TX$ and if this is different from $0$ we know that *OLS* can be applied and we get weights for our features.\n",
        "\n",
        "```r\n",
        "det(t(X)%*%X)\n",
        "model\n",
        "```\n",
        "\n",
        "\n",
        "What would happen if we had more features in the data?\n",
        "\n",
        "   Dist to School |    Property Area   |    Housing Area   |      Value   |    Property Tax |  Sales Price\n",
        "------------------ | ----------------   |   -------------   |   --------   |   ------------- | ------------\n",
        "               7.0 |              0.4   |           1800    |        234   |             9.8 |        267.5\n",
        "               2.3 |              0.8   |           1980    |        244   |            10.5 |        278.2\n",
        "               4.3 |              1.1   |           2120    |        252   |            16.2 |        284.5\n",
        "               3.8 |              0.6   |           2500    |        280   |            18.4 |        310.4\n",
        "\n",
        "\n",
        "Can we determine the weights by applying the *OLS* model ? Let's see..\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```r\n",
        "X <- matrix(c(7.0,2.3,4.3,3.8,0.4,0.8,1.1,0.6,\n",
        "              1800,1980,2120, 2500, 234,244,252,280,9.8,10.5,16.2,18.4),4,5)\n",
        "X\n",
        "det(t(X)%*%X)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```{r, echo=TRUE}\n",
        "model <- lm(Y ~ X)\n",
        "model\n",
        "```\n",
        "\n",
        "<div class=\"green\">\n",
        "Critical Thinking: \n",
        "\n",
        "- What is the message of this paragraph?\n",
        "\n",
        "- What will happen if we add more features but the number of observations remains the same?\n",
        "</div>\n",
        "\n",
        "## Variable Selection\n",
        "\n",
        "It is pretty clear by now that we need to select at most three variables in order for our multiple linear regression to work by applying the least squares method.\n",
        "\n",
        "How exactly can we choose variables and how many (model) choices do we have?\n",
        "\n",
        "- If our approach is exhaustive, we have in total $2^5=32$ possible sparsity patterns. Imagine how many sparsity patterns we can get if the total number of features is $100.$\n",
        "\n",
        "The main idea is that we do not want to and we do not have all the resources to analyze all possible sparsity patterns, therefore we need to impose some extra restrictions or conditions that will help us select one sparsity pattern or just get the weights for all features.\n",
        "\n",
        "## Regularization\n",
        "\n",
        "One way to achieve variable selection or simply determine reasonable values for all weights is the method *regularization*, which translates to an optimization problem with constraints on the vector of weights.\n",
        "\n",
        "That is to say that we minimize the sum of squared residuals with a *constraint* imposed on the vector of weights:\n",
        "\n",
        "$$\n",
        "\\text{minimize} \\sum_{i=1}^{n}\\left(y_i -\\beta_0-\\sum_{j=1}^{p}X_{ij}\\beta_j \\right)^2\n",
        "$$\n",
        "\n",
        "subject to a constraint on $(\\beta_1,\\beta_2,...\\beta_p)$ such as, for example, $\\sum_{j=0}^{p}|\\beta_j|<C$ or $\\sum_{j=0}^{p}\\beta_j^2<C$\n",
        "\n",
        " Let's consider the following example in R:\n",
        " \n",
        " ```r\n",
        "library(lmridge)\n",
        "model <- lmridge(Y ~ ., as.data.frame(X), K=0.001, scaling=(\"scaled\"))\n",
        "model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEYZHrP-hDEf"
      },
      "source": [
        "What would happen if we had more features in the data?\n",
        "\n",
        "   Dist to School |    Property Area   |    Housing Area   |      Value   |    Property Tax |  Sales Price\n",
        "------------------ | ----------------   |   -------------   |   --------   |   ------------- | ------------\n",
        "               7.0 |              0.4   |           1800    |        234   |             9.8 |        267.5\n",
        "               2.3 |              0.8   |           1980    |        244   |            10.5 |        278.2\n",
        "               4.3 |              1.1   |           2120    |        252   |            16.2 |        284.5\n",
        "               3.8 |              0.6   |           2500    |        280   |            18.4 |        310.4\n",
        "\n",
        "\n",
        "Can we determine the weights by applying the *OLS* model ? Let's see..\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```r\n",
        "X <- matrix(c(7.0,2.3,4.3,3.8,0.4,0.8,1.1,0.6,\n",
        "              1800,1980,2120, 2500, 234,244,252,280,9.8,10.5,16.2,18.4),4,5)\n",
        "X\n",
        "det(t(X)%*%X)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm5oyALchdX-"
      },
      "outputs": [],
      "source": [
        "X = np.array([[7.0,2.3,4.3,3.8],[0.4,0.8,1.1,0.6],\n",
        "              [1800,1980,2120, 2500], [234,244,252,280],[9.8,10.5,16.2,18.4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrKUDHezhfIG"
      },
      "outputs": [],
      "source": [
        "X = np.transpose(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqI1ukMKGEBp"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPDKTWr2hoNC",
        "outputId": "f07a9fa3-7135-45f0-9a33-86173388779a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 5)"
            ]
          },
          "execution_count": 45,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape # the claim is that we NEED variable selection or regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TKHz9wOh5HB",
        "outputId": "fb86c64e-8ed6-43a8-b2f9-3e618e07e9f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.022219667101180705"
            ]
          },
          "execution_count": 46,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the determinant of X' * X is 0\n",
        "np.linalg.det(np.matmul(np.transpose(X),X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUHiJKADicLi"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHRib_Kxih_q",
        "outputId": "a00d8368-b0ad-4691-ba37-abc899f7b0a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ridge(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=None,\n",
              "      normalize=False, random_state=None, solver='auto', tol=0.001)"
            ]
          },
          "execution_count": 48,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr = Ridge(alpha=0.01)\n",
        "lr.fit(X,Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT_rRda2i09_",
        "outputId": "613ddd6f-e089-40b8-9b49-a33e83883c32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.05499038, -0.0880771 ,  0.03861881,  0.4372389 , -0.47123514]])"
            ]
          },
          "execution_count": 49,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we got the regularized coeffiecients by Ridge regression\n",
        "lr.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJCfZKCrwP70",
        "outputId": "4986d9c4-3e24-4be8-98af-4861b59fbe73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: Cython in /usr/local/lib/python3.7/dist-packages (0.29.22)\n",
            "Collecting git+https://github.com/statsmodels/statsmodels\n",
            "  Cloning https://github.com/statsmodels/statsmodels to /tmp/pip-req-build-px_04k_0\n",
            "  Running command git clone -q https://github.com/statsmodels/statsmodels /tmp/pip-req-build-px_04k_0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.0.dev0+266.g6d4d588b0) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.0.dev0+266.g6d4d588b0) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.0.dev0+266.g6d4d588b0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.0.dev0+266.g6d4d588b0) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels==0.13.0.dev0+266.g6d4d588b0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels==0.13.0.dev0+266.g6d4d588b0) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->statsmodels==0.13.0.dev0+266.g6d4d588b0) (1.15.0)\n",
            "Building wheels for collected packages: statsmodels\n",
            "  Building wheel for statsmodels (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for statsmodels: filename=statsmodels-0.13.0.dev0+266.g6d4d588b0-cp37-cp37m-linux_x86_64.whl size=17998708 sha256=f10762e8de3148393f924dc481f44596637bbe6e33920d4e6addf4416fba3f5d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hg6ie4lf/wheels/7d/ad/45/ac1a03bd759c2fa74c486e2b1950d94b55f511b4c2b0418bd5\n",
            "Successfully built statsmodels\n",
            "Installing collected packages: statsmodels\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed statsmodels-0.13.0.dev0+266.g6d4d588b0\n"
          ]
        }
      ],
      "source": [
        "# This is important: update the statsmodels package\n",
        "! pip install --upgrade Cython\n",
        "! pip install --upgrade git+https://github.com/statsmodels/statsmodels\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw2v10cQ7C2h"
      },
      "outputs": [],
      "source": [
        "# general imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import ceil\n",
        "from scipy import linalg\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "BJcivn5LeIHY",
        "outputId": "a9f20185-4088-4b42-9bc3-46b6d23d79ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>town</th>\n",
              "      <th>tract</th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>crime</th>\n",
              "      <th>residential</th>\n",
              "      <th>industrial</th>\n",
              "      <th>river</th>\n",
              "      <th>nox</th>\n",
              "      <th>rooms</th>\n",
              "      <th>older</th>\n",
              "      <th>distance</th>\n",
              "      <th>highway</th>\n",
              "      <th>tax</th>\n",
              "      <th>ptratio</th>\n",
              "      <th>lstat</th>\n",
              "      <th>cmedv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nahant</td>\n",
              "      <td>2011</td>\n",
              "      <td>-70.955002</td>\n",
              "      <td>42.255001</td>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>no</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.199997</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296</td>\n",
              "      <td>15.300000</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Swampscott</td>\n",
              "      <td>2021</td>\n",
              "      <td>-70.949997</td>\n",
              "      <td>42.287498</td>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>no</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.900002</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.799999</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Swampscott</td>\n",
              "      <td>2022</td>\n",
              "      <td>-70.935997</td>\n",
              "      <td>42.283001</td>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>no</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.099998</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.799999</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.700001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Marblehead</td>\n",
              "      <td>2031</td>\n",
              "      <td>-70.928001</td>\n",
              "      <td>42.292999</td>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>no</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.799999</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.700001</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.400002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Marblehead</td>\n",
              "      <td>2032</td>\n",
              "      <td>-70.921997</td>\n",
              "      <td>42.298000</td>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>no</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.200001</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.700001</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.200001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>Winthrop</td>\n",
              "      <td>1801</td>\n",
              "      <td>-70.986000</td>\n",
              "      <td>42.231201</td>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>no</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.099998</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>9.67</td>\n",
              "      <td>22.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>Winthrop</td>\n",
              "      <td>1802</td>\n",
              "      <td>-70.990997</td>\n",
              "      <td>42.227501</td>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>no</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.699997</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>9.08</td>\n",
              "      <td>20.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>Winthrop</td>\n",
              "      <td>1803</td>\n",
              "      <td>-70.994797</td>\n",
              "      <td>42.226002</td>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>no</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>5.64</td>\n",
              "      <td>23.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>Winthrop</td>\n",
              "      <td>1804</td>\n",
              "      <td>-70.987503</td>\n",
              "      <td>42.223999</td>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>no</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.300003</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>6.48</td>\n",
              "      <td>22.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>Winthrop</td>\n",
              "      <td>1805</td>\n",
              "      <td>-70.982498</td>\n",
              "      <td>42.221001</td>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>no</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.800003</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>7.88</td>\n",
              "      <td>19.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows Ã— 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           town  tract  longitude   latitude  ...  tax    ptratio  lstat      cmedv\n",
              "0        Nahant   2011 -70.955002  42.255001  ...  296  15.300000   4.98  24.000000\n",
              "1    Swampscott   2021 -70.949997  42.287498  ...  242  17.799999   9.14  21.600000\n",
              "2    Swampscott   2022 -70.935997  42.283001  ...  242  17.799999   4.03  34.700001\n",
              "3    Marblehead   2031 -70.928001  42.292999  ...  222  18.700001   2.94  33.400002\n",
              "4    Marblehead   2032 -70.921997  42.298000  ...  222  18.700001   5.33  36.200001\n",
              "..          ...    ...        ...        ...  ...  ...        ...    ...        ...\n",
              "501    Winthrop   1801 -70.986000  42.231201  ...  273  21.000000   9.67  22.400000\n",
              "502    Winthrop   1802 -70.990997  42.227501  ...  273  21.000000   9.08  20.600000\n",
              "503    Winthrop   1803 -70.994797  42.226002  ...  273  21.000000   5.64  23.900000\n",
              "504    Winthrop   1804 -70.987503  42.223999  ...  273  21.000000   6.48  22.000000\n",
              "505    Winthrop   1805 -70.982498  42.221001  ...  273  21.000000   7.88  19.000000\n",
              "\n",
              "[506 rows x 17 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('drive/MyDrive/Colab Notebooks/Boston Housing Prices.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMCkMlgDjskW"
      },
      "source": [
        "Extract the features we want (specifying them without any criteria is not optimal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeBpYgM1eenu"
      },
      "outputs": [],
      "source": [
        "features = ['crime','rooms','residential','industrial','nox','older','distance','highway','tax','ptratio','lstat']\n",
        "X = np.array(df[features])\n",
        "y = np.array(df['cmedv']).reshape(-1,1)\n",
        "dat = np.concatenate([X,y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDw6o1_kmkBT",
        "outputId": "ab639558-5448-4023-803c-8cc1e08f231d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(506, 11)"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnigJyhefQpt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split as tts\n",
        "dat_train, dat_test = tts(dat, test_size=0.3, random_state=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjjIdU51cToi",
        "outputId": "e7d3b0d1-5b6a-4404-cdd1-70dbc4d0a710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE Linear Model = $3,640.02\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lm = LinearRegression()\n",
        "lm.fit(dat_train[:,:-1],dat_train[:,-1])\n",
        "yhat_lm = lm.predict(dat_test[:,:-1])\n",
        "mae_lm = mean_absolute_error(dat_test[:,-1], yhat_lm)\n",
        "print(\"MAE Linear Model = ${:,.2f}\".format(1000*mae_lm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xOdbJpzp_ck"
      },
      "source": [
        "### Critical Thinking Question: Is this the best we can do with a linear model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwintZPdp-X3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm3rXNJtuRJW"
      },
      "outputs": [],
      "source": [
        "dat_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVrZZURu3Vcu"
      },
      "source": [
        "## Neural Network Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no_Y4SpkaZqY"
      },
      "outputs": [],
      "source": [
        "# imports for creating a Neural Networks\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import r2_score\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pabI8Tz1-wGB",
        "outputId": "a5eb8814-e6ab-4256-aefb-baf2d6cc3137"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(354, 11)"
            ]
          },
          "execution_count": 23,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dat_train[:,:-1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK1gXIBX7A3F"
      },
      "outputs": [],
      "source": [
        "# Create a Neural Network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation=\"relu\", input_dim=11))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(8, activation=\"relu\"))\n",
        "# Since the regression is performed, a Dense layer containing a single neuron with a linear activation function.\n",
        "# Typically ReLu-based activation are used but since it is performed regression, it is needed a linear activation.\n",
        "model.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "# Compile model: The model is initialized with the Adam optimizer and then it is compiled.\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-3, decay=1e-3 / 200))\n",
        "\n",
        "# Patient early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=800)\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(dat_train[:,:-1], dat_train[:,11], validation_split=0.3, epochs=1000, batch_size=100, verbose=0, callbacks=[es])\n",
        "\n",
        "# Calculate predictions\n",
        "#yhat_nn = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_uDLN75b2qH"
      },
      "source": [
        "## Here are the predictions we made for the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUbUsyGh-nRw",
        "outputId": "3663bd78-ad17-4dbd-90ca-bf4ca8a8838b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE Neural Network = $2,652.57\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "yhat_nn = model.predict(dat_test[:,:-1])\n",
        "mae_nn = mean_absolute_error(dat_test[:,-1], yhat_nn)\n",
        "print(\"MAE Neural Network = ${:,.2f}\".format(1000*mae_nn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-fji3Nt-owj"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,9))\n",
        "ax.set_xlim(3, 9)\n",
        "ax.set_ylim(0, 51)\n",
        "ax.scatter(x=df['rooms'], y=df['cmedv'],s=25)\n",
        "ax.plot(X_test, lm.predict(X_test), color='red',label='Linear Regression')\n",
        "ax.plot(dat_test[:,0], yhat_nn, color='lightgreen',lw=2.5,label='Neural Network')\n",
        "ax.plot(dat_test[:,0], model_lowess(dat_train,dat_test,Epanechnikov,0.53), color='orange',lw=2.5,label='Kernel Weighted Regression')\n",
        "ax.set_xlabel('Number of Rooms',fontsize=16,color='navy')\n",
        "ax.set_ylabel('House Price (Thousands of Dollars)',fontsize=16,color='navy')\n",
        "ax.set_title('Boston Housing Prices',fontsize=16,color='purple')\n",
        "ax.grid(b=True,which='major', color ='grey', linestyle='-', alpha=0.8)\n",
        "ax.grid(b=True,which='minor', color ='grey', linestyle='--', alpha=0.2)\n",
        "ax.minorticks_on()\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fObIlRhKJFy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqzyt7BARDCl"
      },
      "outputs": [],
      "source": [
        "kf = KFold(n_splits=10, shuffle=True, random_state=1693)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWdGHA9TRHVw",
        "outputId": "ac7dbdaf-d274-45fb-8617-742ad3241d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validated MAE Linear Regression = $4,447.94\n"
          ]
        }
      ],
      "source": [
        "mae_lm = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  X_train = dat[idxtrain,0]\n",
        "  y_train = dat[idxtrain,1]\n",
        "  X_test  = dat[idxtest,0]\n",
        "  y_test = dat[idxtest,1]\n",
        "  lm.fit(X_train.reshape(-1,1),y_train)\n",
        "  yhat_lm = lm.predict(X_test.reshape(-1,1))\n",
        "  mae_lm.append(mean_absolute_error(y_test, yhat_lm))\n",
        "print(\"Validated MAE Linear Regression = ${:,.2f}\".format(1000*np.mean(mae_lm)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1yCP16cgFSz",
        "outputId": "68768f49-ad4a-4f61-ed46-e686cdddd663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validated MAE Local Kernel Regression = $4,090.03\n",
            "Validated MAE Local Kernel Regression = $4,090.03\n",
            "Validated MAE Local Kernel Regression = $4,090.03\n",
            "1 loop, best of 3: 531 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "\n",
        "mae_lk = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  dat_test = dat[idxtest,:]\n",
        "  y_test = dat_test[np.argsort(dat_test[:, 0]),1]\n",
        "  yhat_lk = model_lowess(dat[idxtrain,:],dat[idxtest,:],Gaussian,0.15)\n",
        "  mae_lk.append(mean_absolute_error(y_test, yhat_lk))\n",
        "print(\"Validated MAE Local Kernel Regression = ${:,.2f}\".format(1000*np.mean(mae_lk)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_CUmGMZR0Tk",
        "outputId": "456934f3-024b-4d5e-d6e8-a6f81407beed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00501: early stopping\n",
            "Epoch 00506: early stopping\n",
            "Epoch 00539: early stopping\n",
            "Epoch 00800: early stopping\n",
            "Epoch 00672: early stopping\n",
            "Epoch 00689: early stopping\n",
            "Epoch 00551: early stopping\n",
            "Validated MAE Neural Network Regression = $5,108.02\n"
          ]
        }
      ],
      "source": [
        "#%%timeit -n 1\n",
        "\n",
        "mae_nn = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  X_train = dat[idxtrain,0]\n",
        "  y_train = dat[idxtrain,1]\n",
        "  X_test  = dat[idxtest,0]\n",
        "  y_test = dat[idxtest,1]\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=500)\n",
        "  model.fit(X_train.reshape(-1,1),y_train,validation_split=0.3, epochs=1000, batch_size=100, verbose=0, callbacks=[es])\n",
        "  yhat_nn = model.predict(X_test.reshape(-1,1))\n",
        "  mae_nn.append(mean_absolute_error(y_test, yhat_nn))\n",
        "print(\"Validated MAE Neural Network Regression = ${:,.2f}\".format(1000*np.mean(mae_nn)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mom_T9x8bUHl"
      },
      "source": [
        "## XGBoost\n",
        "\n",
        "The method is related to Random Forest\n",
        "\n",
        "https://towardsdatascience.com/xgboost-python-example-42777d01001e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3iQle1ER-cI"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwEh4jc3wKyw"
      },
      "outputs": [],
      "source": [
        "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO9TMxFBwcen",
        "outputId": "b346e654-36ba-4bf1-ef71-ec11c0806790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validated MAE XGBoost Regression = $4,179.17\n",
            "Validated MAE XGBoost Regression = $4,179.17\n",
            "Validated MAE XGBoost Regression = $4,179.17\n",
            "1 loop, best of 3: 181 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "\n",
        "mae_xgb = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  X_train = dat[idxtrain,0]\n",
        "  y_train = dat[idxtrain,1]\n",
        "  X_test  = dat[idxtest,0]\n",
        "  y_test = dat[idxtest,1]\n",
        "  model_xgb.fit(X_train.reshape(-1,1),y_train)\n",
        "  yhat_xgb = model_xgb.predict(X_test.reshape(-1,1))\n",
        "  mae_xgb.append(mean_absolute_error(y_test, yhat_xgb))\n",
        "print(\"Validated MAE XGBoost Regression = ${:,.2f}\".format(1000*np.mean(mae_xgb)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk4DCf7-rzEy"
      },
      "outputs": [],
      "source": [
        "cstring = 'c'*364"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQyF8aP3r6_"
      },
      "source": [
        "## Using Kernel Regression from StatsModels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZhrdVXTs9mL"
      },
      "outputs": [],
      "source": [
        "hello = 'c'*dat_train_poly.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-Qm9aaFVs9xW",
        "outputId": "2806c44a-981f-49bc-b4ab-9a8f7566f109"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP_wvqM9thfi"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQTBZ5cttoZF"
      },
      "outputs": [],
      "source": [
        "poly = PolynomialFeatures(degree=2)\n",
        "dat_train_poly = poly.fit_transform(dat_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obBU-bGuwsKq"
      },
      "outputs": [],
      "source": [
        "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
        "\n",
        "\n",
        "model_KernReg = KernelReg(endog=dat_train[:,-1],exog=dat_train_poly,var_type=hello,ckertype='gaussian')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f47Z4YZtmcc"
      },
      "outputs": [],
      "source": [
        "# Implementation of stepwise regression\n",
        "def stepwise_selection(X, y, \n",
        "                       initial_list=[], \n",
        "                       threshold_in=0.01, \n",
        "                       threshold_out = 0.05, \n",
        "                       verbose=True):\n",
        "    \"\"\" Perform a forward-backward feature selection \n",
        "    based on p-value from statsmodels.api.OLS\n",
        "    Arguments:\n",
        "        X - pandas.DataFrame with candidate features\n",
        "        y - list-like with the target\n",
        "        initial_list - list of features to start with (column names of X)\n",
        "        threshold_in - include a feature if its p-value < threshold_in\n",
        "        threshold_out - exclude a feature if its p-value > threshold_out\n",
        "        verbose - whether to print the sequence of inclusions and exclusions\n",
        "    Returns: list of selected features \n",
        "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
        "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details \"\"\"\n",
        "    \n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed=False\n",
        "        # forward step\n",
        "        excluded = list(set(X.columns)-set(included))\n",
        "        new_pval = pd.Series(index=excluded)\n",
        "        for new_column in excluded:\n",
        "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
        "            new_pval[new_column] = model.pvalues[new_column]\n",
        "        best_pval = new_pval.min()\n",
        "        if best_pval < threshold_in:\n",
        "            best_feature = new_pval.idxmin()\n",
        "            included.append(best_feature)\n",
        "            changed=True\n",
        "            if verbose:\n",
        "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
        "\n",
        "        # backward step\n",
        "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
        "        # use all coefs except intercept\n",
        "        pvalues = model.pvalues.iloc[1:]\n",
        "        worst_pval = pvalues.max() # null if pvalues is empty\n",
        "        if worst_pval > threshold_out:\n",
        "            changed=True\n",
        "            worst_feature = pvalues.idxmax()\n",
        "            included.remove(worst_feature)\n",
        "            if verbose:\n",
        "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
        "        if not changed:\n",
        "            break\n",
        "    return included"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0mERUpTwIWV"
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame(data=dat_train[:,:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7pFZTq2vzrU",
        "outputId": "c6f4a49d-5921-4b82-9065-c679fb04e443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Add                              10 with p-value 2.95846e-61\n",
            "Add                               1 with p-value 5.47201e-14\n",
            "Add                               9 with p-value 2.50685e-11\n",
            "Add                               6 with p-value 0.000788183\n",
            "Add                               4 with p-value 8.34391e-08\n",
            "Add                               2 with p-value 0.00268086\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[10, 1, 9, 6, 4, 2]"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stepwise_selection(df_train,dat_train[:,-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q57pk5wtiJ0",
        "outputId": "18ad45d0-461a-459a-f988-906c55a7ea23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(354, 1)"
            ]
          },
          "execution_count": 43,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dat_train[:,-1].reshape(-1,1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0fq_s0_CLMI"
      },
      "outputs": [],
      "source": [
        "yhat_sm_test, y_std = model_KernReg.fit(dat_test[:,:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geqRpN8siZ1W",
        "outputId": "0842b17b-6a46-469e-e4b9-e731221a5ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE StatsModels Kernel Regression = $2,854.57\n"
          ]
        }
      ],
      "source": [
        "mae_sm = mean_absolute_error(dat_test[:,-1], yhat_sm_test)\n",
        "print(\"MAE StatsModels Kernel Regression = ${:,.2f}\".format(1000*mae_sm))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "DATA 410  Lecture 9 - Spring 2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
